<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="2575.4">
  <style type="text/css">
    p.p3 {margin: 0.0px 0.0px 12.0px 0.0px; font: 12.0px Times; -webkit-text-stroke: #000000}
    p.p4 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Times; -webkit-text-stroke: #000000}
    p.p5 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Times; color: #5a5a5a; -webkit-text-stroke: #5a5a5a; min-height: 14.0px}
    span.s1 {font-kerning: none}
  </style>
</head>
<body>
<h1 style="margin: 0.0px 0.0px 16.1px 0.0px; font: 24.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Mathematical Proofs</b><b></b></span></h1>
<h2 style="margin: 0.0px 0.0px 14.9px 0.0px; font: 18.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Proof: Derivative of \( \ln p \)</b><b></b></span></h2>
<p class="p3"><span class="s1">The derivative of \( \ln p \) is defined as:</span></p>
<p class="p4"><span class="s1">\[ \frac{d}{dp} \ln p = \lim_{\Delta p \to 0} \frac{\ln(p + \Delta p) - \ln p}{\Delta p} \]</span></p>
<p class="p3"><span class="s1">Using the logarithm difference rule:</span></p>
<p class="p4"><span class="s1">\[ \ln(p + \Delta p) - \ln p = \ln \left( \frac{p + \Delta p}{p} \right) \]</span></p>
<p class="p3"><span class="s1">Rewriting the derivative:</span></p>
<p class="p4"><span class="s1">\[ \frac{d}{dp} \ln p = \lim_{\Delta p \to 0} \frac{\ln(1 + \frac{\Delta p}{p})}{\Delta p} \]</span></p>
<p class="p3"><span class="s1">Using the limit property:</span></p>
<p class="p4"><span class="s1">\[ \lim_{x \to 0} \frac{\ln(1 + x)}{x} = 1 \]</span></p>
<p class="p3"><span class="s1">where we set \( x = \frac{\Delta p}{p} \), we get:</span></p>
<p class="p4"><span class="s1">\[ \ln(1 + \frac{\Delta p}{p}) \approx \frac{\Delta p}{p} \]</span></p>
<p class="p3"><span class="s1">Thus, our derivative simplifies to:</span></p>
<p class="p4"><span class="s1">\[ \frac{d}{dp} \ln p = \frac{1}{p} \]</span></p>
<p class="p5"><span class="s1"></span><br></p>
<h2 style="margin: 0.0px 0.0px 14.9px 0.0px; font: 18.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Proof: Maximizing Entropy Using Lagrange Multipliers</b><b></b></span></h2>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 1: Define the Entropy Function</b><b></b></span></h3>
<p class="p4"><span class="s1">\[ H(p_1, p_2, \dots, p_n) = - \sum_{i=1}^{n} p_i \ln p_i \]</span></p>
<p class="p3"><span class="s1">with the constraint:</span></p>
<p class="p4"><span class="s1">\[ \sum_{i=1}^{n} p_i = 1 \]</span></p>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 2: Construct the Lagrange Function</b><b></b></span></h3>
<p class="p4"><span class="s1">\[ \mathcal{L}(p_1, p_2, \dots, p_n, \lambda) = - \sum_{i=1}^{n} p_i \ln p_i + \lambda \left( \sum_{i=1}^{n} p_i - 1 \right) \]</span></p>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 3: Differentiate the Entropy Term</b><b></b></span></h3>
<p class="p4"><span class="s1">\[ \frac{\partial}{\partial p_i} (- p_i \ln p_i) = - (\ln p_i + 1) \]</span></p>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 4: Differentiate the Lagrange Constraint</b><b></b></span></h3>
<p class="p4"><span class="s1">\[ \frac{\partial}{\partial p_i} \lambda \sum_{i=1}^{n} p_i = \lambda \]</span></p>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 5: Solve for \( p_i \)</b><b></b></span></h3>
<p class="p4"><span class="s1">\[ - (\ln p_i + 1) + \lambda = 0 \] \[ \ln p_i = \lambda - 1 \] \[ p_i = e^{\lambda - 1} \]</span></p>
<p class="p3"><span class="s1">Since \( \sum p_i = 1 \), we solve:</span></p>
<p class="p4"><span class="s1">\[ n e^{\lambda - 1} = 1 \] \[ e^{\lambda - 1} = \frac{1}{n} \] \[ p_i = \frac{1}{n}, \quad \forall i \]</span></p>
<p class="p3"><span class="s1">This confirms entropy is maximized when all probabilities are equal.</span></p>
<p class="p5"><span class="s1"></span><br></p>
<h2 style="margin: 0.0px 0.0px 14.9px 0.0px; font: 18.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Confirming Maximum Entropy Using Taylor Series</b><b></b></span></h2>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 1: Define Small Deviations</b><b></b></span></h3>
<p class="p3"><span class="s1">Let:</span></p>
<p class="p4"><span class="s1">\[ p_i = \frac{1}{n} + \delta_i, \quad \text{where} \quad \sum_{i=1}^{n} \delta_i = 0 \]</span></p>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 2: Taylor Expansion of \( H(p) \)</b><b></b></span></h3>
<p class="p3"><span class="s1">Expanding entropy using a second-order Taylor series:</span></p>
<p class="p4"><span class="s1">\[ H(p + \delta) = H(p^*) + \sum_{i} \frac{\partial H}{\partial p_i} \Big|_{p^*} (p_i - p_i^*) + \frac{1}{2} \sum_{i,j} \frac{\partial^2 H}{\partial p_i \partial p_j} \Big|_{p^*} (p_i - p_i^*) (p_j - p_j^*) + O(\delta^3) \]</span></p>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 3: Compute the First-Order Term</b><b></b></span></h3>
<p class="p4"><span class="s1">\[ \frac{\partial H}{\partial p_i} = - (\ln p_i + 1) \]</span></p>
<p class="p3"><span class="s1">At \( p_i^* = 1/n \):</span></p>
<p class="p4"><span class="s1">\[ \frac{\partial H}{\partial p_i} \Big|_{p^*} = \ln n - 1 \]</span></p>
<p class="p3"><span class="s1">Since \( \sum (p_i - p_i^*) = 0 \), this term vanishes.</span></p>
<h3 style="margin: 0.0px 0.0px 14.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000"><span class="s1"><b>Step 4: Compute the Second-Order Term</b><b></b></span></h3>
<p class="p4"><span class="s1">\[ \frac{\partial^2 H}{\partial p_i^2} = -\frac{1}{p_i} \]</span></p>
<p class="p3"><span class="s1">At \( p_i^* = 1/n \):</span></p>
<p class="p4"><span class="s1">\[ \frac{\partial^2 H}{\partial p_i^2} = -n \]</span></p>
<p class="p3"><span class="s1">So the second-order term is:</span></p>
<p class="p4"><span class="s1">\[ \frac{1}{2} \sum_i (-n) (p_i - p_i^*)^2 = -\frac{n}{2} \sum_i (p_i - p_i^*)^2 \]</span></p>
<p class="p3"><span class="s1">Since \(-\ frac{n}{2} \sum_i (p_i - p_i^*)^2 \ geq 0 \), this term is **always negative**, confirming **concavity**.</span></p>
<p class="p3"><span class="s1">Thus, entropy is maximized at \( p_i = 1/n \)</span></p>
</body>
</html>
