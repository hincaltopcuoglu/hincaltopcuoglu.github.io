<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Maximum Entropy with Lagrange Multipliers</title>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; padding: 40px; line-height: 1.6; max-width: 800px; margin: auto; }
    code { background: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
    pre { background: #f4f4f4; padding: 12px; overflow-x: auto; border-radius: 4px; }
  </style>
</head>
<body>

<h2>ðŸ“˜ Maximum Entropy with Lagrange Multipliers</h2>

<p>We want to find the discrete probability distribution \( \{p_i\}_{i=1}^n \) that maximizes Shannon entropy:</p>

<p>\[ H(p) = -\sum_{i=1}^n p_i \log p_i \]</p>

<p>Subject to:</p>
<ul>
  <li>Normalization: \( \sum_{i=1}^n p_i = 1 \)</li>
  <li>Expectation constraint: \( \sum_{i=1}^n p_i x_i = M \)</li>
</ul>

<hr>

<h3>ðŸª® Step 1: Lagrangian Formulation</h3>

<p>We define the Lagrangian with multipliers \( \lambda \) and \( \theta \):</p>

<p>\[
\mathcal{L}(p_1, \dots, p_n, \lambda, \theta) =
-\sum_{i=1}^n p_i \log p_i
- \lambda \left( \sum_{i=1}^n p_i - 1 \right)
- \theta \left( \sum_{i=1}^n p_i x_i - M \right)
\]</p>

<hr>

<h3>ðŸ§  Step 2: Take Derivatives</h3>

<p>With respect to \( p_i \):</p>

<p>\[ \frac{\partial \mathcal{L}}{\partial p_i} = -\log p_i - 1 - \lambda - \theta x_i = 0 \]</p>

<p>Solving:</p>

<p>\[ p_i = \exp(-1 - \lambda - \theta x_i) \]</p>

<p>Use the partition function:</p>

<p>\[ Z = \sum_{j=1}^n \exp(-\theta x_j) \quad \Rightarrow \quad p_i = \frac{1}{Z} \exp(-\theta x_i) \]</p>

<hr>

<h3>âœ… Final Result</h3>

<p>\[ \boxed{p_i = \frac{1}{Z} \exp(-\theta x_i)} \quad \text{where} \quad Z = \sum_{j=1}^n \exp(-\theta x_j) \]</p>

<p>We solve for \( \theta \) using:</p>

<p>\[ \sum_{i=1}^n p_i x_i = M \]</p>

<hr>

<h2>ðŸ‘‰ Python Code Implementation</h2>

<pre><code>import numpy as np
from scipy.optimize import root_scalar

def max_entropy_vs_random(n=10, M_target=5.5):
    \"\"\"
    Compares entropy of a random distribution vs. the distribution
    that maximizes entropy under a given expectation constraint.
    \"\"\"
    x = np.arange(1, n + 1)

    # 1. Generate a random distribution
    rand_vals = np.random.rand(n)
    p_random = rand_vals / np.sum(rand_vals)
    entropy_random = -np.sum(p_random * np.log(p_random))
    expected_random = np.sum(p_random * x)

    print(\"ðŸŽ² Random Distribution:\")
    for i, pi in enumerate(p_random, 1):
        print(f\"  p{i} = {pi:.6f}\")
    print(f\"  âˆ‘p_i = {np.sum(p_random):.6f}\")
    print(f\"  âˆ‘p_i * x_i = {expected_random:.6f}\")
    print(f\"  Entropy = {entropy_random:.6f}\\n\")

    # 2. Solve for maximum entropy distribution
    def constraint(theta):
        exp_terms = np.exp(-theta * x)
        Z = np.sum(exp_terms)
        p = exp_terms / Z
        return np.sum(p * x) - M_target

    sol = root_scalar(constraint, bracket=[-10, 10], method='brentq')
    theta = sol.root

    exp_terms = np.exp(-theta * x)
    Z = np.sum(exp_terms)
    p_opt = exp_terms / Z
    entropy_opt = -np.sum(p_opt * np.log(p_opt))
    expected_opt = np.sum(p_opt * x)
    C = 1 / Z
    lambda_val = -1 - np.log(C)

    print(\"ðŸ“ˆ Maximum Entropy Distribution:\")
    for i, pi in enumerate(p_opt, 1):
        print(f\"  p{i} = {pi:.6f}\")
    print(f\"  Î¸ = {theta:.6f}\")
    print(f\"  Î» = {lambda_val:.6f}\")
    print(f\"  âˆ‘p_i = {np.sum(p_opt):.6f}\")
    print(f\"  âˆ‘p_i * x_i = {expected_opt:.6f} (target M = {M_target})\")
    print(f\"  Entropy = {entropy_opt:.6f}\")

    print(\"\\nðŸ“Š Entropy Comparison:\")
    print(f\"  Random Entropy  = {entropy_random:.6f}\")
    print(f\"  Maximum Entropy = {entropy_opt:.6f}\")
    print(f\"  Difference      = {entropy_opt - entropy_random:.6f}\")
</code></pre>

<hr>

<h2>ðŸ“Œ What Did We Do?</h2>
<ul>
  <li>âœ… Defined and formulated the entropy maximization problem</li>
  <li>âœ… Used Lagrange multipliers to impose constraints</li>
  <li>âœ… Derived: \( p_i = \frac{1}{Z} \exp(-\theta x_i) \)</li>
  <li>âœ… Solved for \( \theta \) numerically</li>
  <li>âœ… Compared entropy of random vs optimal distribution</li>
</ul>

<hr>

<h2>ðŸ’¡ Why Maximum Entropy Matters</h2>
<p>Maximum entropy gives us the <strong>least biased</strong> distribution under known information:</p>
<ul>
  <li>It respects known constraints (like the mean)</li>
  <li>It assumes nothing more</li>
  <li>It avoids hidden bias and overfitting</li>
</ul>

<h3>ðŸ§  Applications</h3>
<ul>
  <li><strong>Physics</strong>: Boltzmann distribution</li>
  <li><strong>Machine Learning</strong>: Logistic regression, MaxEnt models</li>
  <li><strong>Statistics</strong>: Uninformative Bayesian priors</li>
  <li><strong>NLP & Signal Processing</strong>: Text and image restoration</li>
</ul>

<blockquote>Maximum entropy is how we model uncertainty <strong>honestly</strong>, using only what we know.</blockquote>

</body>
</html>
