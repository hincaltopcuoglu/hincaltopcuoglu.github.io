
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Maximum Entropy with Lagrange Multipliers</title>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\(', '\)']],
        displayMath: [['\[', '\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; padding: 40px; line-height: 1.6; max-width: 800px; margin: auto; }
    code { background: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
    pre { background: #f4f4f4; padding: 12px; overflow-x: auto; border-radius: 4px; }
  </style>
</head>
<body>

<h2>ðŸ“˜ Maximum Entropy with Lagrange Multipliers</h2>

<p>We want to find the discrete probability distribution \( \{p_i\}_{i=1}^n \) that maximizes Shannon entropy:</p>

<p>\[ H(p) = -\sum_{i=1}^n p_i \log p_i \]</p>

<p>Subject to:</p>
<ul>
  <li>Normalization: \( \sum_{i=1}^n p_i = 1 \)</li>
  <li>Expectation constraint: \( \sum_{i=1}^n p_i x_i = M \)</li>
</ul>

<hr>

<h3>ðŸª® Step 1: Lagrangian Formulation</h3>

<p>We define the Lagrangian with multipliers \( \lambda \) and \( 	heta \):</p>

<p>\[
\mathcal{L}(p_1, \dots, p_n, \lambda, 	heta) =
-\sum_{i=1}^n p_i \log p_i
- \lambda \left( \sum_{i=1}^n p_i - 1 
ight)
- 	heta \left( \sum_{i=1}^n p_i x_i - M 
ight)
\]</p>

<hr>

<h3>ðŸ§  Step 2: Take Derivatives</h3>

<p>With respect to \( p_i \):</p>

<p>\[ rac{\partial \mathcal{L}}{\partial p_i} = -\log p_i - 1 - \lambda - 	heta x_i = 0 \]</p>

<p>Solving:</p>

<p>\[ p_i = \exp(-1 - \lambda - 	heta x_i) \]</p>

<p>Use the partition function:</p>

<p>\[ Z = \sum_{j=1}^n \exp(-	heta x_j) \quad \Rightarrow \quad p_i = rac{1}{Z} \exp(-	heta x_i) \]</p>

<hr>

<h3>âœ… Final Result</h3>

<p>\[ oxed{p_i = rac{1}{Z} \exp(-	heta x_i)} \quad 	ext{where} \quad Z = \sum_{j=1}^n \exp(-	heta x_j) \]</p>

<p>We solve for \( 	heta \) using:</p>

<p>\[ \sum_{i=1}^n p_i x_i = M \]</p>

<hr>

<h2>ðŸ‘‰ Python Code Implementation</h2>

<pre><code># Python-style pseudocode for clarity
import numpy as np
from scipy.optimize import root_scalar

def max_entropy_vs_random(n=10, M_target=5.5):
    x = np.arange(1, n + 1)

    rand_vals = np.random.rand(n)
    p_random = rand_vals / rand_vals.sum()
    entropy_random = -np.sum(p_random * np.log(p_random))

    def constraint(theta):
        Z = np.sum(np.exp(-theta * x))
        p = np.exp(-theta * x) / Z
        return np.sum(p * x) - M_target

    sol = root_scalar(constraint, bracket=[-10, 10])
    theta = sol.root
    Z = np.sum(np.exp(-theta * x))
    p_opt = np.exp(-theta * x) / Z
    entropy_opt = -np.sum(p_opt * np.log(p_opt))

    print("Random entropy:", entropy_random)
    print("Maximum entropy:", entropy_opt)
</code></pre>

<hr>

<h2>ðŸ“Œ What Did We Do?</h2>
<ul>
  <li>âœ… Defined and formulated the entropy maximization problem</li>
  <li>âœ… Used Lagrange multipliers to impose constraints</li>
  <li>âœ… Derived: \( p_i = rac{1}{Z} \exp(-	heta x_i) \)</li>
  <li>âœ… Solved for \( 	heta \) numerically</li>
  <li>âœ… Compared entropy of random vs optimal distribution</li>
</ul>

<hr>

<h2>ðŸ’¡ Why Maximum Entropy Matters</h2>
<p>Maximum entropy gives us the <strong>least biased</strong> distribution under known information:</p>
<ul>
  <li>It respects known constraints (like the mean)</li>
  <li>It assumes nothing more</li>
  <li>It avoids hidden bias and overfitting</li>
</ul>

<h3>ðŸ§  Applications</h3>
<ul>
  <li><strong>Physics</strong>: Boltzmann distribution</li>
  <li><strong>Machine Learning</strong>: Logistic regression, MaxEnt models</li>
  <li><strong>Statistics</strong>: Uninformative Bayesian priors</li>
  <li><strong>NLP & Signal Processing</strong>: Text and image restoration</li>
</ul>

<blockquote>Maximum entropy is how we model uncertainty <strong>honestly</strong>, using only what we know.</blockquote>

</body>
</html>
