<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proof of Joint Entropy Formula with Shannon Entropy and Conditional Entropy</title>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>
<body>
    <h1>Proof of the Joint Entropy Formula with Shannon Entropy and Conditional Entropy</h1>

    <p><strong>Goal:</strong> Prove that:</p>
    <p>
        $$ H(X, Y) = H(X) + H(Y | X) $$
    </p>
    <p>Where:</p>
    <ul>
        <li><strong>Joint entropy</strong> \( H(X, Y) \) is the uncertainty of the pair of random variables \( X \) and \( Y \).</li>
        <li><strong>Marginal entropy</strong> \( H(X) \) is the uncertainty of \( X \) alone.</li>
        <li><strong>Conditional entropy</strong> \( H(Y | X) \) is the uncertainty of \( Y \) given that \( X \) is known.</li>
    </ul>

    <h3>Step 1: Express Joint Entropy Using the Chain Rule</h3>
    <p>The joint entropy of \( X \) and \( Y \) can be written using the chain rule as:</p>
    <p>
        $$ H(X, Y) = - \sum_{x \in X, y \in Y} p(x, y) \log p(x, y) $$
    </p>
    <p>Now, using the fact that \( p(x, y) = p(x) p(y | x) \), we can rewrite this as:</p>
    <p>
        $$ H(X, Y) = - \sum_{x \in X, y \in Y} p(x) p(y | x) \log \left( p(x) p(y | x) \right) $$
    </p>
    <p>Using the logarithmic property \( \log(ab) = \log(a) + \log(b) \), we get:</p>
    <p>
        $$ H(X, Y) = - \sum_{x \in X, y \in Y} p(x) p(y | x) \left( \log p(x) + \log p(y | x) \right) $$
    </p>

    <h3>Step 2: Split the Sum</h3>
    <p>Distribute the terms in the sum:</p>
    <p>
        $$ H(X, Y) = - \sum_{x \in X, y \in Y} p(x) p(y | x) \log p(x) - \sum_{x \in X, y \in Y} p(x) p(y | x) \log p(y | x) $$
    </p>

    <h3>Step 3: Simplify the First Term</h3>
    <p>The first term is:</p>
    <p>
        $$ - \sum_{x \in X, y \in Y} p(x) p(y | x) \log p(x) $$
    </p>
    <p>Since \( p(x) \) does not depend on \( y \), we can factor it out:</p>
    <p>
        $$ - \sum_{x \in X} p(x) \log p(x) \sum_{y \in Y} p(y | x) $$
    </p>
    <p>Now, the sum \( \sum_{y \in Y} p(y | x) \) is the total probability distribution of \( Y \) conditioned on \( X = x \). Since \( p(y | x) \) is a conditional probability distribution, we have:</p>
    <p>
        $$ \underline{\sum_{y \in Y} p(y | x) = 1} $$
    </p>
    <p>So, the first term simplifies to:</p>
    <p>
        $$ - \sum_{x \in X} p(x) \log p(x) \times 1 = - \sum_{x \in X} p(x) \log p(x) = H(X) $$
    </p>

    <h3>Step 4: Simplify the Second Term</h3>
    <p>The second term is:</p>
    <p>
        $$ - \sum_{x \in X, y \in Y} p(x) p(y | x) \log p(y | x) $$
    </p>
    <p>Since \( p(x) \) does not depend on \( y \), we can factor it out of the sum over \( y \) to get:</p>
    <p>
        $$ - \sum_{x \in X} p(x) \sum_{y \in Y} p(y | x) \log p(y | x) $$
    </p>
    <p>This is the definition of the conditional entropy \( H(Y | X) \), so we have:</p>
    <p>
        $$ H(Y | X) = - \sum_{x \in X} p(x) \sum_{y \in Y} p(y | x) \log p(y | x) $$
    </p>

    <h3>Step 5: Combine the Results</h3>
    <p>Now we can combine the results from Step 3 and Step 4:</p>
    <p>
        $$ H(X, Y) = H(X) + H(Y | X) $$
    </p>

    <p><strong>Conclusion:</strong> We have shown that:</p>
    <p>
        $$ H(X, Y) = H(X) + H(Y | X) $$
    </p>
</body>
</html>
